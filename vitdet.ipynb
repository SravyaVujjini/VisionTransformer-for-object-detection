{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of the Vision Transformer for Object Detection**"
      ],
      "metadata": {
        "id": "lTy7DE-7e8pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0xC2bXEL-tP",
        "outputId": "f5d8db5b-d60c-40d3-c420-64ccf9f5bd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Libraries**"
      ],
      "metadata": {
        "id": "Q51B2XoJetB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "lSU6B7VsiOcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "Pyy9Kg2-ewfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tomato_base_dir = \"/content/drive/MyDrive/tomato/\"\n",
        "path_images_train = os.path.join(tomato_base_dir, \"train/\")\n",
        "path_annot_train = os.path.join(tomato_base_dir, \"train/_annotations.coco.json\")"
      ],
      "metadata": {
        "id": "HMxQ45uLRU90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate(batch):\n",
        "    images, targets = zip(*batch)\n",
        "\n",
        "    resized_images = [transforms.functional.resize(img, (32, 32)) for img in images]\n",
        "\n",
        "    return resized_images, targets"
      ],
      "metadata": {
        "id": "2lVSOz95dwKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set your paths\n",
        "tomato_base_dir = \"/content/drive/MyDrive/tomato/\"\n",
        "path_images_train = os.path.join(tomato_base_dir, \"train/\")\n",
        "path_annot_train = os.path.join(tomato_base_dir, \"train/_annotations.coco.json\")\n",
        "\n",
        "# Define your transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add other transformations as needed (e.g., normalization, data augmentation)\n",
        "])\n",
        "\n",
        "# Create a CocoDataset using torchvision\n",
        "coco_dataset = CocoDetection(root=path_images_train, annFile=path_annot_train, transform=transform)\n",
        "\n",
        "# Define a DataLoader\n",
        "trainloader = DataLoader(coco_dataset, batch_size=32, shuffle=True,collate_fn=custom_collate)\n",
        "\n",
        "# Get one batch of images and annotations\n",
        "dataiter = iter(trainloader)\n",
        "images, targets = next(dataiter)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKC_lmNJZyhl",
        "outputId": "8e1e80aa-4892-408e-b1f8-9f6fc6428432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dividing the images into patches**"
      ],
      "metadata": {
        "id": "ncWYBJ7O70Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_image_into_patches(images, patch_size):\n",
        "    batch_size, num_channels, height, width = images.shape\n",
        "    num_vert_patches = height // patch_size\n",
        "    num_horiz_patches = width // patch_size\n",
        "\n",
        "    patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
        "    patches = patches.reshape(batch_size, num_channels, num_vert_patches*num_horiz_patches, patch_size, patch_size)\n",
        "    patches = patches.permute(0, 2, 1, 3, 4)\n",
        "    patches = patches.reshape(batch_size, num_vert_patches*num_horiz_patches, num_channels*patch_size*patch_size)\n",
        "\n",
        "    return patches"
      ],
      "metadata": {
        "id": "bWZ1y3aydEog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizing the patches**"
      ],
      "metadata": {
        "id": "CTFLxRGq74tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch of images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Get one image from the batch\n",
        "image = images[0]\n",
        "\n",
        "# Split the image into patches\n",
        "patches = split_image_into_patches(image.unsqueeze(0), 8)\n",
        "\n",
        "# Calculate the number of patches in each dimension to create a 4x4 grid of 4x4 patches\n",
        "num_patches = patches.shape[1]\n",
        "num_cols = 4\n",
        "num_rows = 4\n",
        "\n",
        "# Display the original image\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(np.moveaxis(image.numpy(), 0, -1))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Display the patches in a grid format\n",
        "plt.figure(figsize=(4,4))\n",
        "for i in range(num_rows):\n",
        "    for j in range(num_cols):\n",
        "        plt.subplot(num_rows, num_cols, i*num_cols+j+1)\n",
        "        plt.imshow(np.transpose(patches[0, i*num_cols+j].reshape(3,8,8), (1,2,0)))\n",
        "        plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "nFkih_oCdVX6",
        "outputId": "39dbb16e-ef7a-4490-b0a8-6dd3f87f1f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function that takes the flattened patches and projects it to the embedding dimension**"
      ],
      "metadata": {
        "id": "KyHUBeNq8Eul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def embed_patches(patches, embedding_dim):\n",
        "\n",
        "    # Define the linear embedding layer\n",
        "    embed_layer = nn.Linear(in_features=patches.shape[-1], out_features=embedding_dim)\n",
        "\n",
        "    # Apply the embedding layer to the flattened tensor\n",
        "    embed_patches = embed_layer(patches)  # shape: [num_images * num_patches, embedding_dim]\n",
        "\n",
        "    return embed_patches"
      ],
      "metadata": {
        "id": "oe-W4-DEfPfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function to add the positional encoding to the embedding patches**\n"
      ],
      "metadata": {
        "id": "jMh7nWIx8Op5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_position_encoding(embeddings):\n",
        "    num_images, num_patches, embedding_dim = embeddings.shape\n",
        "    seq_length = num_patches * num_images\n",
        "    num_patches = num_patches\n",
        "    num_images = num_images\n",
        "\n",
        "    pos_encoding = torch.zeros(seq_length, embedding_dim)\n",
        "    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, dtype=torch.float) * (-math.log(10000.0) / embedding_dim))\n",
        "    pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "    pos_encoding = pos_encoding[:num_patches, :].unsqueeze(0).repeat(num_images, 1, 1)\n",
        "    pos_encoding = pos_encoding.reshape(-1, pos_encoding.size(-1))\n",
        "    pos_encoding = pos_encoding.view(num_images, num_patches, embedding_dim)\n",
        "\n",
        "    return pos_encoding\n"
      ],
      "metadata": {
        "id": "pOPmFmVpfbNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoder block function with Multi-head attention and the fully connected network**\n"
      ],
      "metadata": {
        "id": "dg6cCWkL8Tvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Encoder block with multi-head attention\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 6*embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(6*embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer normalization\n",
        "        x_norm = self.norm1(x)\n",
        "\n",
        "        # Multi-head attention with residual connection\n",
        "        attn_output, _ = self.mha(x_norm, x_norm, x_norm)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # Layer normalization\n",
        "        x_norm = self.norm2(x)\n",
        "\n",
        "        # Feed-forward network with residual connection\n",
        "        ffn_output = self.ffn(x_norm)\n",
        "        x = x + ffn_output\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-gCHYDk-f07T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MLP Classifer Head**"
      ],
      "metadata": {
        "id": "kP-Xqa2_8a6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        # out = self.relu(out)\n",
        "        # out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "lR6l0OQZiA8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation of the ViT base Model for object Detection**"
      ],
      "metadata": {
        "id": "iM571ILv8fM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTObjectDetector(nn.Module):\n",
        "\n",
        "    def __init__(self, inputs, patch_size, embed_dim, num_heads, mlp_head_units, hidden_size_classifier, num_classes_classifier, device):\n",
        "        super(ViTObjectDetector, self).__init__()\n",
        "\n",
        "        self.inputs = inputs\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_head_units = mlp_head_units\n",
        "        self.hidden_size_classifier = hidden_size_classifier\n",
        "        self.num_classes_classifier = num_classes_classifier\n",
        "        self.device = device\n",
        "\n",
        "        # Create class token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, self.embed_dim))\n",
        "\n",
        "        # Create Encoder blocks\n",
        "        encoder_blocks = []\n",
        "        for i in range(2):\n",
        "            encoder_blocks.append(EncoderBlock(self.embed_dim, self.num_heads))\n",
        "        self.encoder_block = nn.Sequential(*encoder_blocks)\n",
        "\n",
        "        # MLP head for object detection\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(2176, mlp_head_units),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(mlp_head_units, 4)  # Final four neurons that output bounding box\n",
        "        )\n",
        "\n",
        "        # MLP Classifier\n",
        "        self.classifier = MLPClassifier(\n",
        "            input_size=2176,\n",
        "            hidden_size=hidden_size_classifier,\n",
        "            num_classes=num_classes_classifier\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Split image into patches\n",
        "        patches = split_image_into_patches(inputs, self.patch_size)\n",
        "\n",
        "        # Embed patches\n",
        "        embed_layer = embed_patches(patches, self.embed_dim)\n",
        "        embedded_tokens = torch.stack([torch.vstack((self.class_token, embed_layer[i])) for i in range(len(embed_layer))])\n",
        "\n",
        "        # Get position encoding\n",
        "        pos_encoding = get_position_encoding(embedded_tokens)\n",
        "\n",
        "        # Add position encoding to embedded patches\n",
        "        embedded_patches = embedded_tokens + pos_encoding\n",
        "\n",
        "        # Process through Encoder blocks\n",
        "        encoded_sequence = self.encoder_block(embedded_patches)\n",
        "\n",
        "        # Flatten for MLP\n",
        "        flattened_representation = encoded_sequence.flatten(1)\n",
        "\n",
        "        # Apply MLP for object detection\n",
        "        bounding_box = self.mlp_head(flattened_representation)\n",
        "\n",
        "        # Apply MLP Classifier\n",
        "        classification_output = self.classifier(flattened_representation)\n",
        "\n",
        "        return bounding_box, classification_output\n",
        "\n"
      ],
      "metadata": {
        "id": "4wABWHyijtsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ViTObjectDetector(inputs = (32,3,32,32), patch_size=8, embed_dim = 128, num_heads= 2, mlp_head_units = 512, hidden_size_classifier=256,\n",
        "    num_classes_classifier=3 , device=device )\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhXNUOyRjtos",
        "outputId": "64973158-843c-44e8-8757-a54052e36c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTObjectDetector(\n",
              "  (encoder_block): Sequential(\n",
              "    (0): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (1): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp_head): Sequential(\n",
              "    (0): Linear(in_features=2176, out_features=512, bias=True)\n",
              "    (1): GELU(approximate='none')\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              "  (classifier): MLPClassifier(\n",
              "    (fc1): Linear(in_features=2176, out_features=256, bias=True)\n",
              "    (relu): ReLU()\n",
              "    (fc2): Linear(in_features=256, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    }
  ]
}