{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of the Vision Transformer for Object Detection**"
      ],
      "metadata": {
        "id": "lTy7DE-7e8pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0xC2bXEL-tP",
        "outputId": "842c9cf1-6c6a-4e56-fcd6-6b2cb7287644"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Libraries**"
      ],
      "metadata": {
        "id": "Q51B2XoJetB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "lSU6B7VsiOcL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "Pyy9Kg2-ewfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tomato_base_dir = \"/content/drive/MyDrive/tomato/\"\n",
        "path_images_train = os.path.join(tomato_base_dir, \"train/\")\n",
        "path_annot_train = os.path.join(tomato_base_dir, \"train/_annotations.coco.json\")"
      ],
      "metadata": {
        "id": "HMxQ45uLRU90"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate(batch):\n",
        "    images, targets = zip(*batch)\n",
        "\n",
        "    resized_images = [transforms.functional.resize(img, (32, 32)) for img in images]\n",
        "\n",
        "    return resized_images, targets"
      ],
      "metadata": {
        "id": "2lVSOz95dwKN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set your paths\n",
        "tomato_base_dir = \"/content/drive/MyDrive/tomato/\"\n",
        "path_images_train = os.path.join(tomato_base_dir, \"train/\")\n",
        "path_annot_train = os.path.join(tomato_base_dir, \"train/_annotations.coco.json\")\n",
        "\n",
        "# Define your transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add other transformations as needed (e.g., normalization, data augmentation)\n",
        "])\n",
        "\n",
        "# Create a CocoDataset using torchvision\n",
        "coco_dataset = CocoDetection(root=path_images_train, annFile=path_annot_train, transform=transform)\n",
        "\n",
        "# Define a DataLoader\n",
        "trainloader = DataLoader(coco_dataset, batch_size=32, shuffle=True,collate_fn=custom_collate)\n",
        "\n",
        "# Get one batch of images and annotations\n",
        "dataiter = iter(trainloader)\n",
        "images, targets = next(dataiter)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKC_lmNJZyhl",
        "outputId": "3e98e2d8-9431-4e0a-a987-47bac1feafe8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dividing the images into patches**"
      ],
      "metadata": {
        "id": "ncWYBJ7O70Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_image_into_patches(images, patch_size):\n",
        "    batch_size, num_channels, height, width = images.shape\n",
        "    num_vert_patches = height // patch_size\n",
        "    num_horiz_patches = width // patch_size\n",
        "\n",
        "    patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
        "    patches = patches.reshape(batch_size, num_channels, num_vert_patches*num_horiz_patches, patch_size, patch_size)\n",
        "    patches = patches.permute(0, 2, 1, 3, 4)\n",
        "    patches = patches.reshape(batch_size, num_vert_patches*num_horiz_patches, num_channels*patch_size*patch_size)\n",
        "\n",
        "    return patches"
      ],
      "metadata": {
        "id": "bWZ1y3aydEog"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizing the patches**"
      ],
      "metadata": {
        "id": "CTFLxRGq74tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch of images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Get one image from the batch\n",
        "image = images[0]\n",
        "\n",
        "# Split the image into patches\n",
        "patches = split_image_into_patches(image.unsqueeze(0), 8)\n",
        "\n",
        "# Calculate the number of patches in each dimension to create a 4x4 grid of 4x4 patches\n",
        "num_patches = patches.shape[1]\n",
        "num_cols = 4\n",
        "num_rows = 4\n",
        "\n",
        "# Display the original image\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(np.moveaxis(image.numpy(), 0, -1))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Display the patches in a grid format\n",
        "plt.figure(figsize=(4,4))\n",
        "for i in range(num_rows):\n",
        "    for j in range(num_cols):\n",
        "        plt.subplot(num_rows, num_cols, i*num_cols+j+1)\n",
        "        plt.imshow(np.transpose(patches[0, i*num_cols+j].reshape(3,8,8), (1,2,0)))\n",
        "        plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "nFkih_oCdVX6",
        "outputId": "adf68c89-455c-474c-89af-98421e5d73cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function that takes the flattened patches and projects it to the embedding dimension**"
      ],
      "metadata": {
        "id": "KyHUBeNq8Eul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def embed_patches(patches, embedding_dim):\n",
        "\n",
        "    # Define the linear embedding layer\n",
        "    embed_layer = nn.Linear(in_features=patches.shape[-1], out_features=embedding_dim)\n",
        "\n",
        "    # Apply the embedding layer to the flattened tensor\n",
        "    embed_patches = embed_layer(patches)  # shape: [num_images * num_patches, embedding_dim]\n",
        "\n",
        "    return embed_patches"
      ],
      "metadata": {
        "id": "oe-W4-DEfPfC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function to add the positional encoding to the embedding patches**\n"
      ],
      "metadata": {
        "id": "jMh7nWIx8Op5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_position_encoding(embeddings):\n",
        "    num_images, num_patches, embedding_dim = embeddings.shape\n",
        "    seq_length = num_patches * num_images\n",
        "    num_patches = num_patches\n",
        "    num_images = num_images\n",
        "\n",
        "    pos_encoding = torch.zeros(seq_length, embedding_dim)\n",
        "    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, dtype=torch.float) * (-math.log(10000.0) / embedding_dim))\n",
        "    pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "    pos_encoding = pos_encoding[:num_patches, :].unsqueeze(0).repeat(num_images, 1, 1)\n",
        "    pos_encoding = pos_encoding.reshape(-1, pos_encoding.size(-1))\n",
        "    pos_encoding = pos_encoding.view(num_images, num_patches, embedding_dim)\n",
        "\n",
        "    return pos_encoding\n"
      ],
      "metadata": {
        "id": "pOPmFmVpfbNL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoder block function with Multi-head attention and the fully connected network**\n"
      ],
      "metadata": {
        "id": "dg6cCWkL8Tvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Encoder block with multi-head attention\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 6*embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(6*embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer normalization\n",
        "        x_norm = self.norm1(x)\n",
        "\n",
        "        # Multi-head attention with residual connection\n",
        "        attn_output, _ = self.mha(x_norm, x_norm, x_norm)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # Layer normalization\n",
        "        x_norm = self.norm2(x)\n",
        "\n",
        "        # Feed-forward network with residual connection\n",
        "        ffn_output = self.ffn(x_norm)\n",
        "        x = x + ffn_output\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-gCHYDk-f07T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MLP Classifer Head**"
      ],
      "metadata": {
        "id": "kP-Xqa2_8a6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        # out = self.relu(out)\n",
        "        # out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "lR6l0OQZiA8I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation of the ViT base Model for object Detection**"
      ],
      "metadata": {
        "id": "iM571ILv8fM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTObjectDetector(nn.Module):\n",
        "\n",
        "    def __init__(self, inputs, patch_size, embed_dim, num_heads, mlp_head_units, hidden_size_classifier, num_classes_classifier, device):\n",
        "        super(ViTObjectDetector, self).__init__()\n",
        "\n",
        "        self.inputs = inputs\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_head_units = mlp_head_units\n",
        "        self.hidden_size_classifier = hidden_size_classifier\n",
        "        self.num_classes_classifier = num_classes_classifier\n",
        "        self.device = device\n",
        "\n",
        "        # Create class token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, self.embed_dim))\n",
        "\n",
        "        # Create Encoder blocks\n",
        "        encoder_blocks = []\n",
        "        for i in range(12):\n",
        "            encoder_blocks.append(EncoderBlock(self.embed_dim, self.num_heads))\n",
        "        self.encoder_block = nn.Sequential(*encoder_blocks)\n",
        "\n",
        "        # MLP head for object detection\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(2176, mlp_head_units),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(mlp_head_units, 4)  # Final four neurons that output bounding box\n",
        "        )\n",
        "\n",
        "        # MLP Classifier\n",
        "        self.classifier = MLPClassifier(\n",
        "            input_size=2176,\n",
        "            hidden_size=hidden_size_classifier,\n",
        "            num_classes=num_classes_classifier\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Split image into patches\n",
        "        patches = split_image_into_patches(inputs, self.patch_size)\n",
        "\n",
        "        # Embed patches\n",
        "        embed_layer = embed_patches(patches, self.embed_dim)\n",
        "        embedded_tokens = torch.stack([torch.vstack((self.class_token, embed_layer[i])) for i in range(len(embed_layer))])\n",
        "\n",
        "        # Get position encoding\n",
        "        pos_encoding = get_position_encoding(embedded_tokens)\n",
        "\n",
        "        # Add position encoding to embedded patches\n",
        "        embedded_patches = embedded_tokens + pos_encoding\n",
        "\n",
        "        # Process through Encoder blocks\n",
        "        encoded_sequence = self.encoder_block(embedded_patches)\n",
        "\n",
        "        # Flatten for MLP\n",
        "        flattened_representation = encoded_sequence.flatten(1)\n",
        "\n",
        "        # Apply MLP for object detection\n",
        "        bounding_box = self.mlp_head(flattened_representation)\n",
        "\n",
        "        # Apply MLP Classifier\n",
        "        classification_output = self.classifier(flattened_representation)\n",
        "\n",
        "        return bounding_box, classification_output\n",
        "\n"
      ],
      "metadata": {
        "id": "4wABWHyijtsH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ViTObjectDetector(inputs = (32,3,32,32), patch_size=8, embed_dim = 128, num_heads= 4, mlp_head_units = 512, hidden_size_classifier=512,\n",
        "    num_classes_classifier=3 , device=device )\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhXNUOyRjtos",
        "outputId": "a78e3412-f690-4d6f-d497-7a06a7b7645a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTObjectDetector(\n",
              "  (encoder_block): Sequential(\n",
              "    (0): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (1): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (2): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (3): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (4): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (5): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (6): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (7): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (8): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (9): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (10): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (11): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp_head): Sequential(\n",
              "    (0): Linear(in_features=2176, out_features=512, bias=True)\n",
              "    (1): GELU(approximate='none')\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              "  (classifier): MLPClassifier(\n",
              "    (fc1): Linear(in_features=2176, out_features=512, bias=True)\n",
              "    (relu): ReLU()\n",
              "    (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, targets in trainloader:\n",
        "  print(targets)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu_rQVn7U-TW",
        "outputId": "c68a063a-9d37-45aa-862e-6e5ba5959e9c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([{'id': 431, 'image_id': 433, 'category_id': 3, 'bbox': [181, 150, 180, 323.5], 'area': 58230, 'segmentation': [], 'iscrowd': 0}], [{'id': 379, 'image_id': 381, 'category_id': 2, 'bbox': [163, 184, 235, 330], 'area': 77550, 'segmentation': [], 'iscrowd': 0}], [{'id': 241, 'image_id': 242, 'category_id': 2, 'bbox': [175, 163, 227.5, 356.5], 'area': 81103.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 448, 'image_id': 451, 'category_id': 1, 'bbox': [181, 157, 252.5, 323.5], 'area': 81683.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 451, 'image_id': 454, 'category_id': 1, 'bbox': [156, 145, 269, 366.5], 'area': 98588.5, 'segmentation': [], 'iscrowd': 0}], [{'id': 261, 'image_id': 262, 'category_id': 2, 'bbox': [158, 125, 250, 398.5], 'area': 99625, 'segmentation': [], 'iscrowd': 0}], [{'id': 382, 'image_id': 384, 'category_id': 1, 'bbox': [173, 95, 229, 363.5], 'area': 83241.5, 'segmentation': [], 'iscrowd': 0}], [{'id': 202, 'image_id': 201, 'category_id': 1, 'bbox': [148, 219, 393.5, 245], 'area': 96407.5, 'segmentation': [], 'iscrowd': 0}], [{'id': 317, 'image_id': 319, 'category_id': 1, 'bbox': [162, 158, 231, 383.5], 'area': 88588.5, 'segmentation': [], 'iscrowd': 0}], [{'id': 228, 'image_id': 229, 'category_id': 1, 'bbox': [139, 182, 334.5, 213.5], 'area': 71415.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 16, 'image_id': 16, 'category_id': 1, 'bbox': [44, 143, 485.5, 291.5], 'area': 141523.25, 'segmentation': [], 'iscrowd': 0}], [{'id': 28, 'image_id': 28, 'category_id': 3, 'bbox': [147, 148, 246, 378.5], 'area': 93111, 'segmentation': [], 'iscrowd': 0}], [{'id': 76, 'image_id': 76, 'category_id': 2, 'bbox': [139, 168, 247.5, 376.5], 'area': 93183.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 400, 'image_id': 402, 'category_id': 1, 'bbox': [125, 185, 414, 250], 'area': 103500, 'segmentation': [], 'iscrowd': 0}], [{'id': 245, 'image_id': 246, 'category_id': 1, 'bbox': [198, 105, 172.5, 321.5], 'area': 55458.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 193, 'image_id': 192, 'category_id': 1, 'bbox': [168, 113, 242.5, 311.5], 'area': 75538.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 10, 'image_id': 10, 'category_id': 2, 'bbox': [164, 192, 202.5, 331.5], 'area': 67128.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 179, 'image_id': 178, 'category_id': 2, 'bbox': [170, 123, 232.5, 381.5], 'area': 88698.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 13, 'image_id': 13, 'category_id': 1, 'bbox': [198, 105, 172.5, 321.5], 'area': 55458.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 283, 'image_id': 282, 'category_id': 1, 'bbox': [189, 150, 190, 326.5], 'area': 62035, 'segmentation': [], 'iscrowd': 0}], [{'id': 199, 'image_id': 198, 'category_id': 2, 'bbox': [145, 108, 249, 431.5], 'area': 107443.5, 'segmentation': [], 'iscrowd': 0}], [{'id': 43, 'image_id': 41, 'category_id': 2, 'bbox': [155, 183, 230, 348.5], 'area': 80155, 'segmentation': [], 'iscrowd': 0}], [{'id': 324, 'image_id': 326, 'category_id': 2, 'bbox': [174, 187, 236, 275], 'area': 64900, 'segmentation': [], 'iscrowd': 0}], [{'id': 271, 'image_id': 270, 'category_id': 3, 'bbox': [166, 147, 219, 321.5], 'area': 70408.5, 'segmentation': [], 'iscrowd': 0}], [{'id': 132, 'image_id': 130, 'category_id': 2, 'bbox': [188, 185, 190, 328.5], 'area': 62415, 'segmentation': [], 'iscrowd': 0}], [{'id': 299, 'image_id': 300, 'category_id': 2, 'bbox': [188, 165, 211, 330], 'area': 69630, 'segmentation': [], 'iscrowd': 0}], [{'id': 376, 'image_id': 378, 'category_id': 1, 'bbox': [168, 113, 242.5, 311.5], 'area': 75538.75, 'segmentation': [], 'iscrowd': 0}], [{'id': 210, 'image_id': 209, 'category_id': 1, 'bbox': [166, 210, 287, 216.5], 'area': 62135.5, 'segmentation': [], 'iscrowd': 0}], [{'id': 243, 'image_id': 244, 'category_id': 3, 'bbox': [175, 127, 209, 376.5], 'area': 78688.5, 'segmentation': [], 'iscrowd': 0}], [{'id': 152, 'image_id': 150, 'category_id': 2, 'bbox': [165, 138, 234, 388.5], 'area': 90909, 'segmentation': [], 'iscrowd': 0}], [{'id': 89, 'image_id': 89, 'category_id': 3, 'bbox': [193, 155, 180, 323.5], 'area': 58230, 'segmentation': [], 'iscrowd': 0}], [{'id': 230, 'image_id': 231, 'category_id': 1, 'bbox': [168, 113, 242.5, 311.5], 'area': 75538.75, 'segmentation': [], 'iscrowd': 0}])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "bbox_criterion = nn.MSELoss()  # You might need a different loss function based on your requirements\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_bbox_loss = 0.0\n",
        "    total_class_loss = 0.0\n",
        "\n",
        "    for images_list, targets_list in trainloader:\n",
        "        # Unpack images from the list\n",
        "        images = torch.stack(images_list).to(device)\n",
        "\n",
        "        # Extract bounding box and class information from targets\n",
        "        # Extract bounding box and class information from targets\n",
        "        bbox_targets = torch.stack([torch.tensor(target[0]['bbox']) if target else torch.zeros(4) for target in targets_list])\n",
        "        class_targets = torch.tensor([target[0]['category_id'] - 1 if target else 0 for target in targets_list])\n",
        "\n",
        "\n",
        "        # Forward pass\n",
        "        bbox_pred, class_pred = model(images)\n",
        "\n",
        "        # Calculate the loss\n",
        "        bbox_loss = bbox_criterion(bbox_pred, bbox_targets.to(device))\n",
        "        class_loss = class_criterion(class_pred, class_targets.to(device))\n",
        "\n",
        "        total_bbox_loss += bbox_loss.item()\n",
        "        total_class_loss += class_loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        total_loss = bbox_loss + class_loss\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print training statistics for each epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Class Loss: {total_class_loss / len(trainloader)}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3TTLMQMWftJ",
        "outputId": "ec716c5e-5a69-40ef-fd92-f344b2d4e838"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Bbox Loss: 20171.278287760415, Class Loss: 9.570457450548808\n",
            "Epoch 2/10, Bbox Loss: 6487.724674479166, Class Loss: 4.777041284243266\n",
            "Epoch 3/10, Bbox Loss: 6335.984895833333, Class Loss: 2.8295247276624043\n",
            "Epoch 4/10, Bbox Loss: 6196.059342447917, Class Loss: 1.974535083770752\n",
            "Epoch 5/10, Bbox Loss: 6324.840299479167, Class Loss: 1.112716011206309\n",
            "Epoch 6/10, Bbox Loss: 6161.612467447917, Class Loss: 1.0371227423350016\n",
            "Epoch 7/10, Bbox Loss: 6170.36630859375, Class Loss: 1.0293525258700054\n",
            "Epoch 8/10, Bbox Loss: 6250.983333333334, Class Loss: 1.1162301381429036\n",
            "Epoch 9/10, Bbox Loss: 6038.5076171875, Class Loss: 1.0928051829338075\n",
            "Epoch 10/10, Bbox Loss: 6125.462272135416, Class Loss: 1.0716545740763346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/tomato/vit_base')"
      ],
      "metadata": {
        "id": "RxZ80OvRW4th"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your paths\n",
        "tomato_base_dir = \"/content/drive/MyDrive/tomato/\"\n",
        "path_images_test = os.path.join(tomato_base_dir, \"test/\")\n",
        "path_annot_test = os.path.join(tomato_base_dir, \"test/_annotations.coco.json\")\n",
        "\n",
        "# Define your transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add other transformations as needed (e.g., normalization, data augmentation)\n",
        "])\n",
        "\n",
        "# Create a CocoDataset using torchvision\n",
        "coco_dataset_test = CocoDetection(root=path_images_test, annFile=path_annot_test, transform=transform)\n",
        "\n",
        "# Define a DataLoader\n",
        "testloader = DataLoader(coco_dataset_test, batch_size=68, shuffle=True,collate_fn=custom_collate)\n",
        "\n",
        "# Get one batch of images and annotations\n",
        "dataiter_test = iter(testloader)\n",
        "images_test, targets_test = next(dataiter_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cdVRuShXJVL",
        "outputId": "0c12458d-f6a7-453e-bd9b-e1203e1af69f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/tomato/vit_base'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad0WqMjeXnkm",
        "outputId": "02acab49-80e6-430d-d609-669e5d14b232"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTObjectDetector(\n",
              "  (encoder_block): Sequential(\n",
              "    (0): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (1): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (2): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (3): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (4): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (5): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (6): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (7): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (8): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (9): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (10): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (11): EncoderBlock(\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=768, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp_head): Sequential(\n",
              "    (0): Linear(in_features=2176, out_features=512, bias=True)\n",
              "    (1): GELU(approximate='none')\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              "  (classifier): MLPClassifier(\n",
              "    (fc1): Linear(in_features=2176, out_features=512, bias=True)\n",
              "    (relu): ReLU()\n",
              "    (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# targets_test"
      ],
      "metadata": {
        "id": "YCZC4qXaaAT3"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Iterate through the test data\n",
        "for images_test, targets_test in testloader:\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        images_test = torch.stack(images_test).to(device)\n",
        "        outputs = model(images_test)\n",
        "\n",
        "    # Process the model outputs as needed\n",
        "    predicted_bboxes = outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v75BgL3aTuy",
        "outputId": "0caa2e22-8170-432c-8085-120d24eee035"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_bboxes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ0iiS45aVrI",
        "outputId": "00560593-2b4d-4c3f-86d2-be0ce9f2a2f0"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[154.5918, 145.1677, 252.5915, 311.2989],\n",
              "        [154.5825, 145.1823, 252.6270, 311.3213],\n",
              "        [154.6422, 145.1891, 252.7245, 311.4163],\n",
              "        [154.5977, 145.1748, 252.5926, 311.2912],\n",
              "        [154.7196, 145.2121, 252.8145, 311.4650],\n",
              "        [154.5969, 145.1715, 252.6223, 311.3131],\n",
              "        [154.5860, 145.1788, 252.6142, 311.3138],\n",
              "        [154.5809, 145.1741, 252.6109, 311.3124],\n",
              "        [154.5404, 145.1367, 252.5422, 311.2368],\n",
              "        [154.6101, 145.2837, 252.8716, 311.6036],\n",
              "        [154.5780, 145.1716, 252.6046, 311.3022],\n",
              "        [154.5773, 145.1630, 252.5809, 311.2749],\n",
              "        [154.6003, 145.1806, 252.6159, 311.3140],\n",
              "        [154.6014, 145.1711, 252.6027, 311.2998],\n",
              "        [154.5752, 145.1721, 252.6071, 311.3030],\n",
              "        [154.5807, 145.1660, 252.5906, 311.2814],\n",
              "        [154.5779, 145.1736, 252.6108, 311.3057],\n",
              "        [154.5869, 145.1645, 252.5980, 311.2877],\n",
              "        [154.5932, 145.1733, 252.6158, 311.3058],\n",
              "        [154.7053, 145.2645, 252.8532, 311.5601],\n",
              "        [154.5938, 145.1968, 252.7426, 311.4322],\n",
              "        [154.7122, 145.2616, 252.8571, 311.5557],\n",
              "        [154.6081, 145.1872, 252.6220, 311.3270],\n",
              "        [154.5835, 145.1725, 252.6171, 311.3157],\n",
              "        [154.7135, 145.2711, 252.8360, 311.5820],\n",
              "        [154.5605, 145.1587, 252.5771, 311.2784],\n",
              "        [154.6081, 145.1948, 252.6393, 311.3402],\n",
              "        [154.5939, 145.1789, 252.6168, 311.3094],\n",
              "        [154.5765, 145.1651, 252.5961, 311.2927],\n",
              "        [154.5327, 145.1387, 252.5210, 311.2048],\n",
              "        [154.5863, 145.1729, 252.5997, 311.2930],\n",
              "        [154.5880, 145.1713, 252.6196, 311.3091],\n",
              "        [154.5899, 145.1716, 252.6225, 311.3141],\n",
              "        [154.5152, 145.1298, 252.5227, 311.2126],\n",
              "        [154.6852, 145.2604, 252.8513, 311.5717],\n",
              "        [154.5297, 145.1357, 252.5294, 311.2247],\n",
              "        [154.5702, 145.1552, 252.5999, 311.2968],\n",
              "        [154.5792, 145.1615, 252.5937, 311.2877],\n",
              "        [154.5815, 145.1685, 252.5840, 311.2874],\n",
              "        [154.5816, 145.1805, 252.6011, 311.2922],\n",
              "        [154.5311, 145.1516, 252.5477, 311.2446],\n",
              "        [154.5607, 145.1612, 252.5782, 311.2773],\n",
              "        [154.5460, 145.1474, 252.5288, 311.2023],\n",
              "        [154.6746, 145.2165, 252.7016, 311.3759],\n",
              "        [154.4743, 145.1303, 252.5424, 311.2713],\n",
              "        [154.6058, 145.1886, 252.6304, 311.3337],\n",
              "        [154.5551, 145.1605, 252.5712, 311.2878],\n",
              "        [154.6014, 145.2147, 252.7759, 311.4629],\n",
              "        [154.5267, 145.1378, 252.5388, 311.2329],\n",
              "        [154.6021, 145.2200, 252.7454, 311.4140],\n",
              "        [154.5862, 145.1730, 252.6143, 311.3062],\n",
              "        [154.5835, 145.1712, 252.6082, 311.3015],\n",
              "        [154.5533, 145.1605, 252.5762, 311.2866],\n",
              "        [154.6013, 145.1725, 252.6216, 311.3098],\n",
              "        [154.6448, 145.1577, 252.6262, 311.3024],\n",
              "        [154.5487, 145.1505, 252.5610, 311.2572],\n",
              "        [154.6125, 145.1889, 252.6326, 311.3363],\n",
              "        [154.5399, 145.1407, 252.5277, 311.2254],\n",
              "        [154.5417, 145.1450, 252.5539, 311.2469],\n",
              "        [154.7615, 145.3162, 252.9331, 311.6208],\n",
              "        [154.5868, 145.1684, 252.6085, 311.3018],\n",
              "        [154.6096, 145.1926, 252.7164, 311.4031],\n",
              "        [154.5830, 145.1646, 252.5946, 311.2892],\n",
              "        [154.6075, 145.1810, 252.6243, 311.3207],\n",
              "        [154.5378, 145.1442, 252.5378, 311.2270],\n",
              "        [154.6293, 145.2673, 252.8295, 311.5456],\n",
              "        [154.5472, 145.1464, 252.5479, 311.2410],\n",
              "        [154.6836, 145.2813, 252.8023, 311.4838]])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_bboxes = outputs[0].cpu().numpy()\n",
        "predicted_bboxes[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS__VgkOc1j-",
        "outputId": "f27c055c-780f-4a31-d0f8-5f9d6fa9f568"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([154.97855, 145.37875, 252.71478, 311.58182], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}